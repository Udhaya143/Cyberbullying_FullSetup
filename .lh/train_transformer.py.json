{
    "sourceFile": "train_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 14,
            "patches": [
                {
                    "date": 1762603532927,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1762604743695,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -105,11 +105,11 @@\n         log(\"üß† Initializing Transformer model...\")\n         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n \n         # Training setup\n-        training_args = TrainingArguments(\n+training_args = TrainingArguments(\n             output_dir=str(models_dir / \"checkpoints\"),\n-            evaluation_strategy=\"epoch\",\n+            eval_strategy=\"epoch\",   # ‚úÖ correct\n             save_strategy=\"epoch\",\n             learning_rate=2e-5,\n             per_device_train_batch_size=8,\n             per_device_eval_batch_size=8,\n"
                },
                {
                    "date": 1762604766787,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -105,11 +105,11 @@\n         log(\"üß† Initializing Transformer model...\")\n         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n \n         # Training setup\n-training_args = TrainingArguments(\n+        training_args = TrainingArguments(\n             output_dir=str(models_dir / \"checkpoints\"),\n-            eval_strategy=\"epoch\",   # ‚úÖ correct\n+            evaluation_strategy=\"epoch\",\n             save_strategy=\"epoch\",\n             learning_rate=2e-5,\n             per_device_train_batch_size=8,\n             per_device_eval_batch_size=8,\n"
                },
                {
                    "date": 1762605032044,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,20 +106,20 @@\n         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n \n         # Training setup\n         training_args = TrainingArguments(\n-            output_dir=str(models_dir / \"checkpoints\"),\n-            evaluation_strategy=\"epoch\",\n-            save_strategy=\"epoch\",\n-            learning_rate=2e-5,\n-            per_device_train_batch_size=8,\n-            per_device_eval_batch_size=8,\n-            num_train_epochs=2,\n-            weight_decay=0.01,\n-            logging_dir=str(models_dir / \"logs\"),\n-            logging_steps=50,\n-            save_total_limit=1,\n-        )\n+    output_dir=str(models_dir / \"checkpoints\"),\n+    eval_strategy=\"epoch\",  # ‚úÖ correct keyword for your Transformers version\n+    save_strategy=\"epoch\",\n+    learning_rate=2e-5,\n+    per_device_train_batch_size=8,\n+    per_device_eval_batch_size=8,\n+    num_train_epochs=2,\n+    weight_decay=0.01,\n+    logging_dir=str(models_dir / \"logs\"),\n+    logging_steps=50,\n+    save_total_limit=1\n+)\n \n         trainer = Trainer(\n             model=model,\n             args=training_args,\n"
                },
                {
                    "date": 1762605429353,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,162 +5,152 @@\n from sklearn.preprocessing import LabelEncoder\n from pathlib import Path\n import os\n import json\n-import time\n-import sys\n+import numpy as np\n \n # ---------------------------------------------------\n-# SETUP PATHS\n+# PATH SETUP\n # ---------------------------------------------------\n BASE_DIR = Path(__file__).resolve().parent\n uploads_dir = BASE_DIR / \"uploads\"\n models_dir = BASE_DIR / \"models\"\n-log_file = BASE_DIR / \"train_status.log\"\n-\n uploads_dir.mkdir(exist_ok=True)\n models_dir.mkdir(exist_ok=True)\n \n-# ---------------------------------------------------\n-# LOGGING FUNCTION (FOR STREAMING)\n-# ---------------------------------------------------\n-def log(message):\n-    \"\"\"Write messages to log file for streaming.\"\"\"\n-    timestamp = time.strftime(\"[%H:%M:%S]\")\n-    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n-        f.write(f\"{timestamp} {message}\\n\")\n-        f.flush()\n-    print(message)\n-    sys.stdout.flush()\n \n-\n # ---------------------------------------------------\n # TRAINING FUNCTION\n # ---------------------------------------------------\n def train_transformer_model(csv_path):\n-    \"\"\"Train transformer model from uploaded CSV and save it inside models/ directory.\"\"\"\n-    log(f\"üìÇ Loading dataset: {csv_path}\")\n+    \"\"\"Train a transformer model on uploaded CSV.\"\"\"\n+    print(f\"üìÇ Loading dataset: {csv_path}\")\n \n     try:\n+        # Load CSV\n         df = pd.read_csv(csv_path)\n         df = df.dropna()\n \n-        log(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n+        print(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n \n-        # Try to automatically find text + label columns\n+        # Auto-detect text and label columns\n         possible_text_cols = [\"text\", \"tweet\", \"comment\", \"message\", \"post\", \"content\", \"body\", \"sentence\"]\n         possible_label_cols = [\"label\", \"category\", \"target\", \"class\", \"sentiment\", \"tag\"]\n \n-        text_col = next((col for col in df.columns if col.lower() in possible_text_cols), None)\n-        label_col = next((col for col in df.columns if col.lower() in possible_label_cols), None)\n+        text_col = next((c for c in df.columns if c.lower() in possible_text_cols), None)\n+        label_col = next((c for c in df.columns if c.lower() in possible_label_cols), None)\n \n-        # Fallback to first/last column if not detected\n+        # Fallback\n         if text_col is None:\n             text_col = df.columns[0]\n-            log(f\"‚ö†Ô∏è Using first column as text: {text_col}\")\n+            print(f\"‚ö†Ô∏è Using first column as text: {text_col}\")\n         if label_col is None:\n             label_col = df.columns[-1]\n-            log(f\"‚ö†Ô∏è Using last column as label: {label_col}\")\n+            print(f\"‚ö†Ô∏è Using last column as label: {label_col}\")\n \n-        log(f\"‚úÖ Using text column: '{text_col}', label column: '{label_col}'\")\n+        print(f\"‚úÖ Using text column: '{text_col}', label column: '{label_col}'\")\n \n         # Encode labels\n         encoder = LabelEncoder()\n         df[label_col] = encoder.fit_transform(df[label_col])\n-        num_labels = len(set(df[label_col]))\n-        log(f\"üß© Detected {num_labels} unique labels\")\n \n-        # Split into train/val sets\n+        print(f\"üß© Detected {len(encoder.classes_)} unique labels\")\n+\n+        # Split data\n         train_texts, val_texts, train_labels, val_labels = train_test_split(\n-            df[text_col].tolist(), df[label_col].tolist(), test_size=0.2, random_state=42\n+            df[text_col].tolist(),\n+            df[label_col].tolist(),\n+            test_size=0.2,\n+            random_state=42\n         )\n \n+        # Load model + tokenizer\n         model_name = \"distilbert-base-uncased\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n \n-        def tokenize_function(examples):\n-            return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n+        def tokenize_function(texts):\n+            return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n \n-        log(\"üîÑ Tokenizing dataset...\")\n+        print(\"üîÑ Tokenizing dataset...\")\n         train_encodings = tokenize_function(train_texts)\n         val_encodings = tokenize_function(val_texts)\n \n         class Dataset(torch.utils.data.Dataset):\n             def __init__(self, encodings, labels):\n                 self.encodings = encodings\n                 self.labels = labels\n \n-            def __len__(self):\n-                return len(self.labels)\n-\n             def __getitem__(self, idx):\n                 return {\n                     \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n                     \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n-                    \"labels\": torch.tensor(self.labels[idx]),\n+                    \"labels\": torch.tensor(int(self.labels[idx])),\n                 }\n \n+            def __len__(self):\n+                return len(self.labels)\n+\n         train_dataset = Dataset(train_encodings, train_labels)\n         val_dataset = Dataset(val_encodings, val_labels)\n \n-        log(\"üß† Initializing Transformer model...\")\n-        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n+        print(\"üß† Initializing Transformer model...\")\n+        model = AutoModelForSequenceClassification.from_pretrained(\n+            model_name, num_labels=len(encoder.classes_)\n+        )\n \n-        # Training setup\n+        # ‚úÖ FIXED: TrainingArguments key issue\n         training_args = TrainingArguments(\n-    output_dir=str(models_dir / \"checkpoints\"),\n-    eval_strategy=\"epoch\",  # ‚úÖ correct keyword for your Transformers version\n-    save_strategy=\"epoch\",\n-    learning_rate=2e-5,\n-    per_device_train_batch_size=8,\n-    per_device_eval_batch_size=8,\n-    num_train_epochs=2,\n-    weight_decay=0.01,\n-    logging_dir=str(models_dir / \"logs\"),\n-    logging_steps=50,\n-    save_total_limit=1\n-)\n+            output_dir=str(models_dir / \"checkpoints\"),\n+            eval_strategy=\"epoch\",\n+            save_strategy=\"epoch\",\n+            learning_rate=2e-5,\n+            per_device_train_batch_size=8,\n+            per_device_eval_batch_size=8,\n+            num_train_epochs=2,\n+            weight_decay=0.01,\n+            logging_dir=str(models_dir / \"logs\"),\n+            logging_steps=50,\n+            save_total_limit=1,\n+        )\n \n         trainer = Trainer(\n             model=model,\n             args=training_args,\n             train_dataset=train_dataset,\n             eval_dataset=val_dataset,\n         )\n \n-        log(\"üöÄ Starting model training...\")\n+        print(\"üöÄ Starting model training...\")\n         trainer.train()\n-        log(\"‚úÖ Training completed!\")\n+        print(\"‚úÖ Training completed!\")\n \n-        # Save trained model\n+        # Save model\n         model_path = models_dir / \"transformer_model\"\n         model_path.mkdir(exist_ok=True, parents=True)\n-\n         model.save_pretrained(model_path)\n         tokenizer.save_pretrained(model_path)\n \n-        # Save label encoder mapping\n+        # ‚úÖ FIXED: Convert NumPy int64 ‚Üí str before saving\n+        encoder_dict = {\n+            str(cls): int(val) for cls, val in zip(encoder.classes_, encoder.transform(encoder.classes_))\n+        }\n         encoder_path = model_path / \"label_encoder.json\"\n-        with open(encoder_path, \"w\", encoding=\"utf-8\") as f:\n-            json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n+        with open(encoder_path, \"w\") as f:\n+            json.dump(encoder_dict, f, indent=2)\n \n-        log(f\"‚úÖ Model and tokenizer saved to: {model_path}\")\n-        log(\"üéâ Transformer training finished successfully!\")\n-\n+        print(f\"‚úÖ Model saved to: {model_path}\")\n         return model_path\n \n     except Exception as e:\n-        log(f\"‚ùå Training failed: {e}\")\n+        print(f\"‚ùå Training failed: {e}\")\n         raise e\n \n \n # ---------------------------------------------------\n-# MANUAL TEST MODE\n+# TEST RUN\n # ---------------------------------------------------\n if __name__ == \"__main__\":\n     sample_csv = uploads_dir / \"sample_data.csv\"\n-    log_file.unlink(missing_ok=True)\n-\n     if sample_csv.exists():\n         train_transformer_model(str(sample_csv))\n     else:\n-        log(\"‚ö†Ô∏è No sample CSV found in uploads/. Please upload your dataset first.\")\n+        print(\"‚ö†Ô∏è No sample CSV found in uploads/. Please upload your dataset first.\")\n"
                },
                {
                    "date": 1762606767833,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,73 +5,52 @@\n from sklearn.preprocessing import LabelEncoder\n from pathlib import Path\n import os\n import json\n-import numpy as np\n \n-# ---------------------------------------------------\n-# PATH SETUP\n-# ---------------------------------------------------\n BASE_DIR = Path(__file__).resolve().parent\n uploads_dir = BASE_DIR / \"uploads\"\n models_dir = BASE_DIR / \"models\"\n uploads_dir.mkdir(exist_ok=True)\n models_dir.mkdir(exist_ok=True)\n \n+LOG_FILE = BASE_DIR / \"train_status.log\"\n \n-# ---------------------------------------------------\n-# TRAINING FUNCTION\n-# ---------------------------------------------------\n+def log_status(message):\n+    \"\"\"Write training status messages to log file and print to terminal.\"\"\"\n+    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n+        f.write(message + \"\\n\")\n+    print(message)\n+\n def train_transformer_model(csv_path):\n-    \"\"\"Train a transformer model on uploaded CSV.\"\"\"\n-    print(f\"üìÇ Loading dataset: {csv_path}\")\n+    \"\"\"Train transformer model and save it inside models/ directory.\"\"\"\n+    log_status(f\"üìÇ Loading dataset: {csv_path}\")\n \n     try:\n-        # Load CSV\n         df = pd.read_csv(csv_path)\n         df = df.dropna()\n+        log_status(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n \n-        print(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n-\n-        # Auto-detect text and label columns\n         possible_text_cols = [\"text\", \"tweet\", \"comment\", \"message\", \"post\", \"content\", \"body\", \"sentence\"]\n         possible_label_cols = [\"label\", \"category\", \"target\", \"class\", \"sentiment\", \"tag\"]\n \n-        text_col = next((c for c in df.columns if c.lower() in possible_text_cols), None)\n-        label_col = next((c for c in df.columns if c.lower() in possible_label_cols), None)\n+        text_col = next((col for col in df.columns if col.lower() in possible_text_cols), df.columns[0])\n+        label_col = next((col for col in df.columns if col.lower() in possible_label_cols), df.columns[-1])\n+        log_status(f\"‚úÖ Using text column: '{text_col}', label column: '{label_col}'\")\n \n-        # Fallback\n-        if text_col is None:\n-            text_col = df.columns[0]\n-            print(f\"‚ö†Ô∏è Using first column as text: {text_col}\")\n-        if label_col is None:\n-            label_col = df.columns[-1]\n-            print(f\"‚ö†Ô∏è Using last column as label: {label_col}\")\n-\n-        print(f\"‚úÖ Using text column: '{text_col}', label column: '{label_col}'\")\n-\n-        # Encode labels\n         encoder = LabelEncoder()\n         df[label_col] = encoder.fit_transform(df[label_col])\n \n-        print(f\"üß© Detected {len(encoder.classes_)} unique labels\")\n-\n-        # Split data\n         train_texts, val_texts, train_labels, val_labels = train_test_split(\n-            df[text_col].tolist(),\n-            df[label_col].tolist(),\n-            test_size=0.2,\n-            random_state=42\n+            df[text_col].tolist(), df[label_col].tolist(), test_size=0.2, random_state=42\n         )\n \n-        # Load model + tokenizer\n         model_name = \"distilbert-base-uncased\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n \n-        def tokenize_function(texts):\n-            return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n+        def tokenize_function(examples):\n+            return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n \n-        print(\"üîÑ Tokenizing dataset...\")\n         train_encodings = tokenize_function(train_texts)\n         val_encodings = tokenize_function(val_texts)\n \n         class Dataset(torch.utils.data.Dataset):\n@@ -82,75 +61,57 @@\n             def __getitem__(self, idx):\n                 return {\n                     \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n                     \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n-                    \"labels\": torch.tensor(int(self.labels[idx])),\n+                    \"labels\": torch.tensor(self.labels[idx]),\n                 }\n \n             def __len__(self):\n                 return len(self.labels)\n \n         train_dataset = Dataset(train_encodings, train_labels)\n         val_dataset = Dataset(val_encodings, val_labels)\n \n-        print(\"üß† Initializing Transformer model...\")\n-        model = AutoModelForSequenceClassification.from_pretrained(\n-            model_name, num_labels=len(encoder.classes_)\n-        )\n+        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(df[label_col])))\n \n-        # ‚úÖ FIXED: TrainingArguments key issue\n         training_args = TrainingArguments(\n             output_dir=str(models_dir / \"checkpoints\"),\n-            eval_strategy=\"epoch\",\n+            evaluation_strategy=\"epoch\",\n             save_strategy=\"epoch\",\n             learning_rate=2e-5,\n             per_device_train_batch_size=8,\n             per_device_eval_batch_size=8,\n             num_train_epochs=2,\n             weight_decay=0.01,\n             logging_dir=str(models_dir / \"logs\"),\n             logging_steps=50,\n-            save_total_limit=1,\n+            save_total_limit=1\n         )\n \n+        log_status(\"üöÄ Starting model training...\")\n+\n         trainer = Trainer(\n             model=model,\n             args=training_args,\n             train_dataset=train_dataset,\n             eval_dataset=val_dataset,\n         )\n \n-        print(\"üöÄ Starting model training...\")\n         trainer.train()\n-        print(\"‚úÖ Training completed!\")\n+        log_status(\"‚úÖ Training completed!\")\n \n-        # Save model\n         model_path = models_dir / \"transformer_model\"\n         model_path.mkdir(exist_ok=True, parents=True)\n         model.save_pretrained(model_path)\n         tokenizer.save_pretrained(model_path)\n \n-        # ‚úÖ FIXED: Convert NumPy int64 ‚Üí str before saving\n-        encoder_dict = {\n-            str(cls): int(val) for cls, val in zip(encoder.classes_, encoder.transform(encoder.classes_))\n-        }\n         encoder_path = model_path / \"label_encoder.json\"\n         with open(encoder_path, \"w\") as f:\n-            json.dump(encoder_dict, f, indent=2)\n+            json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n \n-        print(f\"‚úÖ Model saved to: {model_path}\")\n+        log_status(f\"‚úÖ Model saved to: {model_path}\")\n+        log_status(\"üéâ Transformer training finished successfully!\")\n         return model_path\n \n     except Exception as e:\n-        print(f\"‚ùå Training failed: {e}\")\n+        log_status(f\"‚ùå Training failed: {e}\")\n         raise e\n-\n-\n-# ---------------------------------------------------\n-# TEST RUN\n-# ---------------------------------------------------\n-if __name__ == \"__main__\":\n-    sample_csv = uploads_dir / \"sample_data.csv\"\n-    if sample_csv.exists():\n-        train_transformer_model(str(sample_csv))\n-    else:\n-        print(\"‚ö†Ô∏è No sample CSV found in uploads/. Please upload your dataset first.\")\n"
                },
                {
                    "date": 1762607210873,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,20 +73,20 @@\n \n         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(df[label_col])))\n \n         training_args = TrainingArguments(\n-            output_dir=str(models_dir / \"checkpoints\"),\n-            evaluation_strategy=\"epoch\",\n-            save_strategy=\"epoch\",\n-            learning_rate=2e-5,\n-            per_device_train_batch_size=8,\n-            per_device_eval_batch_size=8,\n-            num_train_epochs=2,\n-            weight_decay=0.01,\n-            logging_dir=str(models_dir / \"logs\"),\n-            logging_steps=50,\n-            save_total_limit=1\n-        )\n+    output_dir=str(models_dir / \"checkpoints\"),\n+    evaluation_strategy=\"epoch\",   # ‚úÖ fixed here\n+    save_strategy=\"epoch\",\n+    learning_rate=2e-5,\n+    per_device_train_batch_size=8,\n+    per_device_eval_batch_size=8,\n+    num_train_epochs=2,\n+    weight_decay=0.01,\n+    logging_dir=str(models_dir / \"logs\"),\n+    logging_steps=50,\n+    save_total_limit=1\n+)\n \n         log_status(\"üöÄ Starting model training...\")\n \n         trainer = Trainer(\n"
                },
                {
                    "date": 1762608103722,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,8 +5,9 @@\n from sklearn.preprocessing import LabelEncoder\n from pathlib import Path\n import os\n import json\n+import inspect\n \n BASE_DIR = Path(__file__).resolve().parent\n uploads_dir = BASE_DIR / \"uploads\"\n models_dir = BASE_DIR / \"models\"\n@@ -72,22 +73,29 @@\n         val_dataset = Dataset(val_encodings, val_labels)\n \n         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(df[label_col])))\n \n-        training_args = TrainingArguments(\n-    output_dir=str(models_dir / \"checkpoints\"),\n-    evaluation_strategy=\"epoch\",   # ‚úÖ fixed here\n-    save_strategy=\"epoch\",\n-    learning_rate=2e-5,\n-    per_device_train_batch_size=8,\n-    per_device_eval_batch_size=8,\n-    num_train_epochs=2,\n-    weight_decay=0.01,\n-    logging_dir=str(models_dir / \"logs\"),\n-    logging_steps=50,\n-    save_total_limit=1\n-)\n+args = {\n+    \"output_dir\": str(models_dir / \"checkpoints\"),\n+    \"learning_rate\": 2e-5,\n+    \"per_device_train_batch_size\": 8,\n+    \"per_device_eval_batch_size\": 8,\n+    \"num_train_epochs\": 2,\n+    \"weight_decay\": 0.01,\n+    \"logging_dir\": str(models_dir / \"logs\"),\n+    \"logging_steps\": 50,\n+    \"save_total_limit\": 1\n+}\n \n+# For backward compatibility\n+sig = inspect.signature(TrainingArguments)\n+if \"evaluation_strategy\" in sig.parameters:\n+    args[\"evaluation_strategy\"] = \"epoch\"\n+else:\n+    args[\"eval_strategy\"] = \"epoch\"\n+\n+training_args = TrainingArguments(**args)\n+\n         log_status(\"üöÄ Starting model training...\")\n \n         trainer = Trainer(\n             model=model,\n"
                },
                {
                    "date": 1762608293513,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,8 +73,9 @@\n         val_dataset = Dataset(val_encodings, val_labels)\n \n         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(df[label_col])))\n \n+# Build training argument dictionary\n args = {\n     \"output_dir\": str(models_dir / \"checkpoints\"),\n     \"learning_rate\": 2e-5,\n     \"per_device_train_batch_size\": 8,\n@@ -85,41 +86,49 @@\n     \"logging_steps\": 50,\n     \"save_total_limit\": 1\n }\n \n-# For backward compatibility\n+# Backward compatibility for transformers versions\n sig = inspect.signature(TrainingArguments)\n if \"evaluation_strategy\" in sig.parameters:\n     args[\"evaluation_strategy\"] = \"epoch\"\n else:\n     args[\"eval_strategy\"] = \"epoch\"\n \n+# Initialize training arguments safely\n training_args = TrainingArguments(**args)\n \n-        log_status(\"üöÄ Starting model training...\")\n+# Log start\n+log_status(\"üöÄ Starting model training...\")\n \n-        trainer = Trainer(\n-            model=model,\n-            args=training_args,\n-            train_dataset=train_dataset,\n-            eval_dataset=val_dataset,\n-        )\n+# Initialize Trainer\n+trainer = Trainer(\n+    model=model,\n+    args=training_args,\n+    train_dataset=train_dataset,\n+    eval_dataset=val_dataset,\n+)\n \n-        trainer.train()\n-        log_status(\"‚úÖ Training completed!\")\n+# Start training\n+try:\n+    trainer.train()\n+    log_status(\"‚úÖ Training completed!\")\n \n-        model_path = models_dir / \"transformer_model\"\n-        model_path.mkdir(exist_ok=True, parents=True)\n-        model.save_pretrained(model_path)\n-        tokenizer.save_pretrained(model_path)\n+    # Save trained model\n+    model_path = models_dir / \"transformer_model\"\n+    model_path.mkdir(exist_ok=True, parents=True)\n+    model.save_pretrained(model_path)\n+    tokenizer.save_pretrained(model_path)\n \n-        encoder_path = model_path / \"label_encoder.json\"\n-        with open(encoder_path, \"w\") as f:\n-            json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n+    # Save label encoder\n+    encoder_path = model_path / \"label_encoder.json\"\n+    with open(encoder_path, \"w\") as f:\n+        json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n \n-        log_status(f\"‚úÖ Model saved to: {model_path}\")\n-        log_status(\"üéâ Transformer training finished successfully!\")\n-        return model_path\n+    log_status(f\"‚úÖ Model saved to: {model_path}\")\n+    log_status(\"üéâ Transformer training finished successfully!\")\n+    return model_path\n \n-    except Exception as e:\n-        log_status(f\"‚ùå Training failed: {e}\")\n-        raise e\n+except Exception as e:\n+    log_status(f\"‚ùå Training failed: {e}\")\n+    print(f\"‚ùå Training failed: {e}\")\n+    raise e\n"
                },
                {
                    "date": 1762608361703,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -74,9 +74,9 @@\n \n         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(df[label_col])))\n \n # Build training argument dictionary\n-args = {\n+        args = {\n     \"output_dir\": str(models_dir / \"checkpoints\"),\n     \"learning_rate\": 2e-5,\n     \"per_device_train_batch_size\": 8,\n     \"per_device_eval_batch_size\": 8,\n@@ -84,9 +84,9 @@\n     \"weight_decay\": 0.01,\n     \"logging_dir\": str(models_dir / \"logs\"),\n     \"logging_steps\": 50,\n     \"save_total_limit\": 1\n-}\n+        }\n \n # Backward compatibility for transformers versions\n sig = inspect.signature(TrainingArguments)\n if \"evaluation_strategy\" in sig.parameters:\n"
                },
                {
                    "date": 1762608388065,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -87,48 +87,48 @@\n     \"save_total_limit\": 1\n         }\n \n # Backward compatibility for transformers versions\n-sig = inspect.signature(TrainingArguments)\n-if \"evaluation_strategy\" in sig.parameters:\n-    args[\"evaluation_strategy\"] = \"epoch\"\n-else:\n-    args[\"eval_strategy\"] = \"epoch\"\n+        sig = inspect.signature(TrainingArguments)\n+        if \"evaluation_strategy\" in sig.parameters:\n+            args[\"evaluation_strategy\"] = \"epoch\"\n+        else:\n+            args[\"eval_strategy\"] = \"epoch\"\n \n-# Initialize training arguments safely\n-training_args = TrainingArguments(**args)\n+        # Initialize training arguments safely\n+        training_args = TrainingArguments(**args)\n \n-# Log start\n-log_status(\"üöÄ Starting model training...\")\n+        # Log start\n+        log_status(\"üöÄ Starting model training...\")\n \n-# Initialize Trainer\n-trainer = Trainer(\n-    model=model,\n-    args=training_args,\n-    train_dataset=train_dataset,\n-    eval_dataset=val_dataset,\n-)\n+        # Initialize Trainer\n+        trainer = Trainer(\n+            model=model,\n+            args=training_args,\n+            train_dataset=train_dataset,\n+            eval_dataset=val_dataset,\n+        )\n \n-# Start training\n-try:\n-    trainer.train()\n-    log_status(\"‚úÖ Training completed!\")\n+        # Start training\n+        try:\n+            trainer.train()\n+            log_status(\"‚úÖ Training completed!\")\n \n-    # Save trained model\n-    model_path = models_dir / \"transformer_model\"\n-    model_path.mkdir(exist_ok=True, parents=True)\n-    model.save_pretrained(model_path)\n-    tokenizer.save_pretrained(model_path)\n+            # Save trained model\n+            model_path = models_dir / \"transformer_model\"\n+            model_path.mkdir(exist_ok=True, parents=True)\n+            model.save_pretrained(model_path)\n+            tokenizer.save_pretrained(model_path)\n \n-    # Save label encoder\n-    encoder_path = model_path / \"label_encoder.json\"\n-    with open(encoder_path, \"w\") as f:\n-        json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n+            # Save label encoder\n+            encoder_path = model_path / \"label_encoder.json\"\n+            with open(encoder_path, \"w\") as f:\n+                json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n \n-    log_status(f\"‚úÖ Model saved to: {model_path}\")\n-    log_status(\"üéâ Transformer training finished successfully!\")\n-    return model_path\n+            log_status(f\"‚úÖ Model saved to: {model_path}\")\n+            log_status(\"üéâ Transformer training finished successfully!\")\n+            return model_path\n \n-except Exception as e:\n-    log_status(f\"‚ùå Training failed: {e}\")\n-    print(f\"‚ùå Training failed: {e}\")\n-    raise e\n+        except Exception as e:\n+            log_status(f\"‚ùå Training failed: {e}\")\n+            print(f\"‚ùå Training failed: {e}\")\n+            raise e\n"
                },
                {
                    "date": 1762608450864,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n def train_transformer_model(csv_path):\n     \"\"\"Train transformer model and save it inside models/ directory.\"\"\"\n     log_status(f\"üìÇ Loading dataset: {csv_path}\")\n \n-    try:\n+        try:\n         df = pd.read_csv(csv_path)\n         df = df.dropna()\n         log_status(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n \n@@ -130,5 +130,5 @@\n \n         except Exception as e:\n             log_status(f\"‚ùå Training failed: {e}\")\n             print(f\"‚ùå Training failed: {e}\")\n-            raise e\n+            raise\n"
                },
                {
                    "date": 1762608521467,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n def train_transformer_model(csv_path):\n     \"\"\"Train transformer model and save it inside models/ directory.\"\"\"\n     log_status(f\"üìÇ Loading dataset: {csv_path}\")\n \n-        try:\n+    try:\n         df = pd.read_csv(csv_path)\n         df = df.dropna()\n         log_status(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n \n@@ -131,4 +131,10 @@\n         except Exception as e:\n             log_status(f\"‚ùå Training failed: {e}\")\n             print(f\"‚ùå Training failed: {e}\")\n             raise\n+            except FileNotFoundError:\n+                log_status(f\"‚ùå CSV file not found: {csv_path}\")\n+                raise\n+            except Exception as e:\n+                log_status(f\"‚ùå Error loading dataset: {e}\")\n+                raise\n"
                },
                {
                    "date": 1762608548326,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n def train_transformer_model(csv_path):\n     \"\"\"Train transformer model and save it inside models/ directory.\"\"\"\n     log_status(f\"üìÇ Loading dataset: {csv_path}\")\n \n-    try:\n+        try:\n         df = pd.read_csv(csv_path)\n         df = df.dropna()\n         log_status(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n \n@@ -131,10 +131,4 @@\n         except Exception as e:\n             log_status(f\"‚ùå Training failed: {e}\")\n             print(f\"‚ùå Training failed: {e}\")\n             raise\n-            except FileNotFoundError:\n-                log_status(f\"‚ùå CSV file not found: {csv_path}\")\n-                raise\n-            except Exception as e:\n-                log_status(f\"‚ùå Error loading dataset: {e}\")\n-                raise\n"
                },
                {
                    "date": 1762608797303,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,56 +1,81 @@\n import pandas as pd\n import torch\n+import inspect\n from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n from sklearn.model_selection import train_test_split\n from sklearn.preprocessing import LabelEncoder\n from pathlib import Path\n import os\n import json\n-import inspect\n \n+# ---------------------------------------------------\n+# PATHS\n+# ---------------------------------------------------\n BASE_DIR = Path(__file__).resolve().parent\n uploads_dir = BASE_DIR / \"uploads\"\n models_dir = BASE_DIR / \"models\"\n+\n uploads_dir.mkdir(exist_ok=True)\n models_dir.mkdir(exist_ok=True)\n \n-LOG_FILE = BASE_DIR / \"train_status.log\"\n \n+# ---------------------------------------------------\n+# LOG STATUS FUNCTION (for UI updates)\n+# ---------------------------------------------------\n def log_status(message):\n-    \"\"\"Write training status messages to log file and print to terminal.\"\"\"\n-    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n+    \"\"\"Append training status messages to a log file.\"\"\"\n+    log_file = BASE_DIR / \"train_log.txt\"\n+    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n         f.write(message + \"\\n\")\n     print(message)\n \n+\n+# ---------------------------------------------------\n+# TRAINING FUNCTION\n+# ---------------------------------------------------\n def train_transformer_model(csv_path):\n     \"\"\"Train transformer model and save it inside models/ directory.\"\"\"\n     log_status(f\"üìÇ Loading dataset: {csv_path}\")\n \n-        try:\n+    try:\n+        # Load and clean CSV\n         df = pd.read_csv(csv_path)\n         df = df.dropna()\n         log_status(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n \n+        # Detect text & label columns automatically\n         possible_text_cols = [\"text\", \"tweet\", \"comment\", \"message\", \"post\", \"content\", \"body\", \"sentence\"]\n         possible_label_cols = [\"label\", \"category\", \"target\", \"class\", \"sentiment\", \"tag\"]\n \n-        text_col = next((col for col in df.columns if col.lower() in possible_text_cols), df.columns[0])\n-        label_col = next((col for col in df.columns if col.lower() in possible_label_cols), df.columns[-1])\n+        text_col = next((col for col in df.columns if col.lower() in possible_text_cols), None)\n+        label_col = next((col for col in df.columns if col.lower() in possible_label_cols), None)\n+\n+        # Fallbacks if columns not matched\n+        if text_col is None:\n+            text_col = df.columns[0]\n+            log_status(f\"‚ö†Ô∏è Using first column as text: {text_col}\")\n+        if label_col is None:\n+            label_col = df.columns[-1]\n+            log_status(f\"‚ö†Ô∏è Using last column as label: {label_col}\")\n+\n         log_status(f\"‚úÖ Using text column: '{text_col}', label column: '{label_col}'\")\n \n+        # Encode labels\n         encoder = LabelEncoder()\n         df[label_col] = encoder.fit_transform(df[label_col])\n \n+        # Split data\n         train_texts, val_texts, train_labels, val_labels = train_test_split(\n             df[text_col].tolist(), df[label_col].tolist(), test_size=0.2, random_state=42\n         )\n \n+        # Tokenizer setup\n         model_name = \"distilbert-base-uncased\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n \n-        def tokenize_function(examples):\n-            return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n+        def tokenize_function(texts):\n+            return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n \n         train_encodings = tokenize_function(train_texts)\n         val_encodings = tokenize_function(val_texts)\n \n@@ -58,77 +83,90 @@\n             def __init__(self, encodings, labels):\n                 self.encodings = encodings\n                 self.labels = labels\n \n+            def __len__(self):\n+                return len(self.labels)\n+\n             def __getitem__(self, idx):\n                 return {\n                     \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n                     \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n                     \"labels\": torch.tensor(self.labels[idx]),\n                 }\n \n-            def __len__(self):\n-                return len(self.labels)\n-\n         train_dataset = Dataset(train_encodings, train_labels)\n         val_dataset = Dataset(val_encodings, val_labels)\n \n-        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(df[label_col])))\n+        # Load model\n+        model = AutoModelForSequenceClassification.from_pretrained(\n+            model_name, num_labels=len(set(df[label_col]))\n+        )\n \n-# Build training argument dictionary\n+        # ---------------------------------------------------\n+        # TRAINING ARGUMENTS (BACKWARD COMPATIBLE)\n+        # ---------------------------------------------------\n         args = {\n-    \"output_dir\": str(models_dir / \"checkpoints\"),\n-    \"learning_rate\": 2e-5,\n-    \"per_device_train_batch_size\": 8,\n-    \"per_device_eval_batch_size\": 8,\n-    \"num_train_epochs\": 2,\n-    \"weight_decay\": 0.01,\n-    \"logging_dir\": str(models_dir / \"logs\"),\n-    \"logging_steps\": 50,\n-    \"save_total_limit\": 1\n+            \"output_dir\": str(models_dir / \"checkpoints\"),\n+            \"learning_rate\": 2e-5,\n+            \"per_device_train_batch_size\": 8,\n+            \"per_device_eval_batch_size\": 8,\n+            \"num_train_epochs\": 2,\n+            \"weight_decay\": 0.01,\n+            \"logging_dir\": str(models_dir / \"logs\"),\n+            \"logging_steps\": 50,\n+            \"save_total_limit\": 1\n         }\n \n-# Backward compatibility for transformers versions\n         sig = inspect.signature(TrainingArguments)\n         if \"evaluation_strategy\" in sig.parameters:\n             args[\"evaluation_strategy\"] = \"epoch\"\n         else:\n             args[\"eval_strategy\"] = \"epoch\"\n \n-        # Initialize training arguments safely\n         training_args = TrainingArguments(**args)\n \n-        # Log start\n         log_status(\"üöÄ Starting model training...\")\n \n-        # Initialize Trainer\n         trainer = Trainer(\n             model=model,\n             args=training_args,\n             train_dataset=train_dataset,\n             eval_dataset=val_dataset,\n         )\n \n-        # Start training\n-        try:\n-            trainer.train()\n-            log_status(\"‚úÖ Training completed!\")\n+        trainer.train()\n+        log_status(\"‚úÖ Training completed!\")\n \n-            # Save trained model\n-            model_path = models_dir / \"transformer_model\"\n-            model_path.mkdir(exist_ok=True, parents=True)\n-            model.save_pretrained(model_path)\n-            tokenizer.save_pretrained(model_path)\n+        # ---------------------------------------------------\n+        # SAVE MODEL & TOKENIZER\n+        # ---------------------------------------------------\n+        model_path = models_dir / \"transformer_model\"\n+        model_path.mkdir(exist_ok=True, parents=True)\n \n-            # Save label encoder\n-            encoder_path = model_path / \"label_encoder.json\"\n-            with open(encoder_path, \"w\") as f:\n-                json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n+        model.save_pretrained(model_path)\n+        tokenizer.save_pretrained(model_path)\n \n-            log_status(f\"‚úÖ Model saved to: {model_path}\")\n-            log_status(\"üéâ Transformer training finished successfully!\")\n-            return model_path\n+        # Save label encoder\n+        encoder_path = model_path / \"label_encoder.json\"\n+        with open(encoder_path, \"w\") as f:\n+            json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n \n-        except Exception as e:\n-            log_status(f\"‚ùå Training failed: {e}\")\n-            print(f\"‚ùå Training failed: {e}\")\n-            raise\n+        log_status(f\"‚úÖ Model saved to: {model_path}\")\n+        log_status(\"üéâ Transformer training finished successfully!\")\n+        return model_path\n+\n+    except Exception as e:\n+        log_status(f\"‚ùå Training failed: {e}\")\n+        print(f\"‚ùå Training failed: {e}\")\n+        raise e\n+\n+\n+# ---------------------------------------------------\n+# MANUAL TEST MODE\n+# ---------------------------------------------------\n+if __name__ == \"__main__\":\n+    sample_csv = uploads_dir / \"sample_data.csv\"\n+    if sample_csv.exists():\n+        train_transformer_model(str(sample_csv))\n+    else:\n+        print(\"‚ö†Ô∏è No sample CSV found in uploads/. Please upload your dataset first.\")\n"
                }
            ],
            "date": 1762603532927,
            "name": "Commit-0",
            "content": "import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom pathlib import Path\nimport os\nimport json\nimport time\nimport sys\n\n# ---------------------------------------------------\n# SETUP PATHS\n# ---------------------------------------------------\nBASE_DIR = Path(__file__).resolve().parent\nuploads_dir = BASE_DIR / \"uploads\"\nmodels_dir = BASE_DIR / \"models\"\nlog_file = BASE_DIR / \"train_status.log\"\n\nuploads_dir.mkdir(exist_ok=True)\nmodels_dir.mkdir(exist_ok=True)\n\n# ---------------------------------------------------\n# LOGGING FUNCTION (FOR STREAMING)\n# ---------------------------------------------------\ndef log(message):\n    \"\"\"Write messages to log file for streaming.\"\"\"\n    timestamp = time.strftime(\"[%H:%M:%S]\")\n    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n        f.write(f\"{timestamp} {message}\\n\")\n        f.flush()\n    print(message)\n    sys.stdout.flush()\n\n\n# ---------------------------------------------------\n# TRAINING FUNCTION\n# ---------------------------------------------------\ndef train_transformer_model(csv_path):\n    \"\"\"Train transformer model from uploaded CSV and save it inside models/ directory.\"\"\"\n    log(f\"üìÇ Loading dataset: {csv_path}\")\n\n    try:\n        df = pd.read_csv(csv_path)\n        df = df.dropna()\n\n        log(f\"‚úÖ CSV Columns Detected: {list(df.columns)}\")\n\n        # Try to automatically find text + label columns\n        possible_text_cols = [\"text\", \"tweet\", \"comment\", \"message\", \"post\", \"content\", \"body\", \"sentence\"]\n        possible_label_cols = [\"label\", \"category\", \"target\", \"class\", \"sentiment\", \"tag\"]\n\n        text_col = next((col for col in df.columns if col.lower() in possible_text_cols), None)\n        label_col = next((col for col in df.columns if col.lower() in possible_label_cols), None)\n\n        # Fallback to first/last column if not detected\n        if text_col is None:\n            text_col = df.columns[0]\n            log(f\"‚ö†Ô∏è Using first column as text: {text_col}\")\n        if label_col is None:\n            label_col = df.columns[-1]\n            log(f\"‚ö†Ô∏è Using last column as label: {label_col}\")\n\n        log(f\"‚úÖ Using text column: '{text_col}', label column: '{label_col}'\")\n\n        # Encode labels\n        encoder = LabelEncoder()\n        df[label_col] = encoder.fit_transform(df[label_col])\n        num_labels = len(set(df[label_col]))\n        log(f\"üß© Detected {num_labels} unique labels\")\n\n        # Split into train/val sets\n        train_texts, val_texts, train_labels, val_labels = train_test_split(\n            df[text_col].tolist(), df[label_col].tolist(), test_size=0.2, random_state=42\n        )\n\n        model_name = \"distilbert-base-uncased\"\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        def tokenize_function(examples):\n            return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n\n        log(\"üîÑ Tokenizing dataset...\")\n        train_encodings = tokenize_function(train_texts)\n        val_encodings = tokenize_function(val_texts)\n\n        class Dataset(torch.utils.data.Dataset):\n            def __init__(self, encodings, labels):\n                self.encodings = encodings\n                self.labels = labels\n\n            def __len__(self):\n                return len(self.labels)\n\n            def __getitem__(self, idx):\n                return {\n                    \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n                    \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n                    \"labels\": torch.tensor(self.labels[idx]),\n                }\n\n        train_dataset = Dataset(train_encodings, train_labels)\n        val_dataset = Dataset(val_encodings, val_labels)\n\n        log(\"üß† Initializing Transformer model...\")\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n        # Training setup\n        training_args = TrainingArguments(\n            output_dir=str(models_dir / \"checkpoints\"),\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            learning_rate=2e-5,\n            per_device_train_batch_size=8,\n            per_device_eval_batch_size=8,\n            num_train_epochs=2,\n            weight_decay=0.01,\n            logging_dir=str(models_dir / \"logs\"),\n            logging_steps=50,\n            save_total_limit=1,\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n        )\n\n        log(\"üöÄ Starting model training...\")\n        trainer.train()\n        log(\"‚úÖ Training completed!\")\n\n        # Save trained model\n        model_path = models_dir / \"transformer_model\"\n        model_path.mkdir(exist_ok=True, parents=True)\n\n        model.save_pretrained(model_path)\n        tokenizer.save_pretrained(model_path)\n\n        # Save label encoder mapping\n        encoder_path = model_path / \"label_encoder.json\"\n        with open(encoder_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), f, indent=2)\n\n        log(f\"‚úÖ Model and tokenizer saved to: {model_path}\")\n        log(\"üéâ Transformer training finished successfully!\")\n\n        return model_path\n\n    except Exception as e:\n        log(f\"‚ùå Training failed: {e}\")\n        raise e\n\n\n# ---------------------------------------------------\n# MANUAL TEST MODE\n# ---------------------------------------------------\nif __name__ == \"__main__\":\n    sample_csv = uploads_dir / \"sample_data.csv\"\n    log_file.unlink(missing_ok=True)\n\n    if sample_csv.exists():\n        train_transformer_model(str(sample_csv))\n    else:\n        log(\"‚ö†Ô∏è No sample CSV found in uploads/. Please upload your dataset first.\")\n"
        }
    ]
}